{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import gzip\n",
    "from PIL import Image\n",
    "from tqdm import tqdm_notebook\n",
    "import os\n",
    "from matplotlib import pyplot as plt\n",
    "from statistics import mode\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'mnist.pkl.gz'\n",
    "f = gzip.open(filename, 'rb')\n",
    "training_data, validation_data, test_data = pickle.load(f, encoding='latin1')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training, validation and testing data for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of MNIST training data: \n",
      "(50000, 784)\n",
      "(50000,)\n",
      "Shapes of MNIST testing data: \n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "Mtraining_data = np.array(training_data[0]);\n",
    "Mtraining_target = np.array(training_data[1]);\n",
    "Mvalidation_data = np.array(validation_data[0]);\n",
    "Mvalidation_target = np.array(validation_data[1]);\n",
    "Mtest_data = np.array(test_data[0]);\n",
    "Mtest_target = np.array(test_data[1]);\n",
    "#Mtraining_data = np.concatenate((Mtraining_data,Mvalidation_data),axis=0);\n",
    "#Mtraining_target = np.concatenate((Mtraining_target,Mvalidation_target),axis=0);\n",
    "print(\"Shapes of MNIST training data: \");\n",
    "print(np.shape(Mtraining_data));\n",
    "print(np.shape(Mtraining_target));\n",
    "#print(np.shape(Mtraining_data))\n",
    "print(\"Shapes of MNIST testing data: \");\n",
    "print(np.shape(Mtest_data));\n",
    "print(np.shape(Mtest_target));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load USPS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "USPSMat  = []\n",
    "USPSTar  = []\n",
    "curPath  = 'USPSdata/Numerals'\n",
    "savedImg = []\n",
    "\n",
    "for j in range(0,10):\n",
    "    curFolderPath = curPath + '/' + str(j)\n",
    "    imgs =  os.listdir(curFolderPath)\n",
    "    for img in imgs:\n",
    "        curImg = curFolderPath + '/' + img\n",
    "        if curImg[-3:] == 'png':\n",
    "            img = Image.open(curImg,'r')\n",
    "            img = img.resize((28, 28))\n",
    "            savedImg = img\n",
    "            imgdata = (255-np.array(img.getdata()))/255\n",
    "            USPSMat.append(imgdata)\n",
    "            USPSTar.append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading training, validation and testing data for USPS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of USPS testing data: \n",
      "(19999, 784)\n",
      "(19999,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shapes of USPS testing data: \");\n",
    "print(np.shape(USPSMat))\n",
    "print(np.shape(USPSTar))\n",
    "USPSMat = np.array(USPSMat);\n",
    "USPSTar = np.array(USPSTar);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Multiclass Logistic Regression Using Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_activation(X,W):\n",
    "    return np.dot(X,W);\n",
    "\n",
    "def get_model(A):\n",
    "    exp_A = np.exp(A);\n",
    "    for i in exp_A:\n",
    "        sum_exp = np.sum(i); \n",
    "        for j in range(len(i)):\n",
    "            i[j] = i[j]/sum_exp;\n",
    "    return exp_A;\n",
    "\n",
    "def get_hot_target(t,y):\n",
    "    i=0;\n",
    "    for j in t:\n",
    "        a = y[i]\n",
    "        j[a] = j[a] + 1;\n",
    "        i = i+1\n",
    "    return t\n",
    "\n",
    "def get_cross_entropy(X,W,Y):\n",
    "    a = np.dot(X,W);\n",
    "    exp_a = np.exp(a);\n",
    "    sum_ex = 0;\n",
    "    for i in exp_a:\n",
    "        sum_ex = np.sum(i);\n",
    "        for j in range(len(i)):\n",
    "            i[j] = i[j]/sum_ex;\n",
    "    k=0;\n",
    "    count = 0;\n",
    "    for i in exp_a:\n",
    "        j = np.argmax(i);\n",
    "        if(j == Y[k]):\n",
    "            count = count+1;\n",
    "        k = k+1;    \n",
    "    Loss = 0\n",
    "    ln_y = -np.log(exp_a);\n",
    "    i=0;\n",
    "    for j in ln_y:\n",
    "        x = Y[i];\n",
    "        Loss = Loss + j[x];\n",
    "        i=i+1;\n",
    "    return (1/len(X))*Loss,float((count*100)/len(X));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent for Softmax Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "915398a0b3ea496b968f4a22533662f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Weights = np.random.rand(784,10);\n",
    "t_values = np.zeros((50000,10));\n",
    "La = 2;\n",
    "learning_rate = 0.000012;\n",
    "t_values = get_hot_target(t_values,Mtraining_target);\n",
    "LTr = [];\n",
    "ATr = [];\n",
    "LV = [];\n",
    "AV = [];\n",
    "LT = [];\n",
    "AT = [];\n",
    "LU = [];\n",
    "AU = [];\n",
    "ite = [];\n",
    "for i in tqdm_notebook(range(0,100)):\n",
    "    Activation = get_activation(Mtraining_data,Weights);\n",
    "    Model = get_model(Activation);\n",
    "    Delta_EW = np.matmul(np.transpose(Model - t_values),Mtraining_data);\n",
    "    La_Delta_EW  = np.dot(La,Weights);\n",
    "    Delta_E = np.add(Delta_EW, np.transpose(La_Delta_EW));\n",
    "    Delta_W = -np.dot(learning_rate,Delta_E);\n",
    "    W_next = Weights + np.transpose(Delta_W);\n",
    "    Weights = W_next;\n",
    "    #LossTr,accuracyTr = get_cross_entropy(Mtraining_data,Weights,Mtraining_target);\n",
    "    #LossV,accuracyV = get_cross_entropy(Mvalidation_data,Weights,Mvalidation_target);\n",
    "    LossT,accuracyT = get_cross_entropy(Mtest_data,Weights,Mtest_target);\n",
    "    LossU,accuracyU = get_cross_entropy(USPSMat,Weights,USPSTar);\n",
    "    #ATr.append(accuracyTr);\n",
    "    #LTr.append(LossTr);\n",
    "    #AV.append(accuracyV);\n",
    "    #LV.append(LossV);\n",
    "    AT.append(accuracyT);\n",
    "    LT.append(LossT);\n",
    "    LU.append(LossU);\n",
    "    AU.append(accuracyU);\n",
    "    ite.append(i);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ite, LT) \n",
    "plt.xlabel('Iterations') \n",
    "plt.ylabel('cross entropy testing Values')   \n",
    "plt.title('cross entropy testing vs Iterations') \n",
    "plt.show()\n",
    "\n",
    "plt.plot(ite, AT) \n",
    "plt.xlabel('Iterations') \n",
    "plt.ylabel('Testing accuracy Values')   \n",
    "plt.title('Testing accuracy vs Iterations') \n",
    "plt.show()\n",
    "#print(A[999]);\n",
    "plt.plot(ite, LU) \n",
    "plt.xlabel('Iterations') \n",
    "plt.ylabel('cross entropy testing Values for USPS')   \n",
    "plt.title('cross entropy testing vs Iterations') \n",
    "plt.show()\n",
    "\n",
    "plt.plot(ite, AU) \n",
    "plt.xlabel('Iterations') \n",
    "plt.ylabel('Testing accuracy Values for USPS')   \n",
    "plt.title('Testing accuracy vs Iterations') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST test data: \n",
      "87.34\n",
      "Accuracy for USPS dataset: \n",
      "27.2063603180159\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy for MNIST test data: \");\n",
    "print(max(AT));\n",
    "print(\"Accuracy for USPS dataset: \");\n",
    "print(max(AU));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes of MNIST training data: \n",
      "(70000, 784)\n",
      "(70000,)\n",
      "Shapes of MNIST testing data: \n",
      "(10000, 784)\n",
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "Mtraining_data = np.concatenate((Mtraining_data,Mvalidation_data),axis=0);\n",
    "Mtraining_target = np.concatenate((Mtraining_target,Mvalidation_target),axis=0);\n",
    "print(\"Shapes of MNIST training data: \");\n",
    "print(np.shape(Mtraining_data));\n",
    "print(np.shape(Mtraining_target));\n",
    "#print(np.shape(Mtraining_data))\n",
    "print(\"Shapes of MNIST testing data: \");\n",
    "print(np.shape(Mtest_data));\n",
    "print(np.shape(Mtest_target));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras DNN Implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 63000 samples, validate on 7000 samples\n",
      "Epoch 1/100\n",
      "63000/63000 [==============================] - 1s 17us/step - loss: 2.1125 - acc: 0.4226 - val_loss: 1.8869 - val_acc: 0.6357\n",
      "Epoch 2/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 1.7247 - acc: 0.6581 - val_loss: 1.5339 - val_acc: 0.7251\n",
      "Epoch 3/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 1.4198 - acc: 0.7276 - val_loss: 1.2552 - val_acc: 0.7957\n",
      "Epoch 4/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 1.1876 - acc: 0.7774 - val_loss: 1.0495 - val_acc: 0.8337\n",
      "Epoch 5/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 1.0183 - acc: 0.8090 - val_loss: 0.9008 - val_acc: 0.8519\n",
      "Epoch 6/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.8953 - acc: 0.8256 - val_loss: 0.7921 - val_acc: 0.8629\n",
      "Epoch 7/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.8039 - acc: 0.8384 - val_loss: 0.7100 - val_acc: 0.8739\n",
      "Epoch 8/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.7339 - acc: 0.8484 - val_loss: 0.6469 - val_acc: 0.8797\n",
      "Epoch 9/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.6790 - acc: 0.8555 - val_loss: 0.5966 - val_acc: 0.8851\n",
      "Epoch 10/100\n",
      "63000/63000 [==============================] - 1s 14us/step - loss: 0.6348 - acc: 0.8614 - val_loss: 0.5563 - val_acc: 0.8884\n",
      "Epoch 11/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.5986 - acc: 0.8662 - val_loss: 0.5230 - val_acc: 0.8917\n",
      "Epoch 12/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.5684 - acc: 0.8704 - val_loss: 0.4952 - val_acc: 0.8957\n",
      "Epoch 13/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.5429 - acc: 0.8739 - val_loss: 0.4716 - val_acc: 0.8971\n",
      "Epoch 14/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.5210 - acc: 0.8763 - val_loss: 0.4515 - val_acc: 0.9000\n",
      "Epoch 15/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.5021 - acc: 0.8790 - val_loss: 0.4341 - val_acc: 0.9014\n",
      "Epoch 16/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.4857 - acc: 0.8815 - val_loss: 0.4191 - val_acc: 0.9039\n",
      "Epoch 17/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.4712 - acc: 0.8839 - val_loss: 0.4056 - val_acc: 0.9046\n",
      "Epoch 18/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.4583 - acc: 0.8857 - val_loss: 0.3937 - val_acc: 0.9067\n",
      "Epoch 19/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.4468 - acc: 0.8875 - val_loss: 0.3831 - val_acc: 0.9080\n",
      "Epoch 20/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.4364 - acc: 0.8894 - val_loss: 0.3736 - val_acc: 0.9090\n",
      "Epoch 21/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.4270 - acc: 0.8907 - val_loss: 0.3651 - val_acc: 0.9104\n",
      "Epoch 22/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.4185 - acc: 0.8921 - val_loss: 0.3572 - val_acc: 0.9124\n",
      "Epoch 23/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.4107 - acc: 0.8929 - val_loss: 0.3500 - val_acc: 0.9131\n",
      "Epoch 24/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.4035 - acc: 0.8944 - val_loss: 0.3434 - val_acc: 0.9151\n",
      "Epoch 25/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3969 - acc: 0.8956 - val_loss: 0.3373 - val_acc: 0.9160\n",
      "Epoch 26/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3907 - acc: 0.8966 - val_loss: 0.3318 - val_acc: 0.9163\n",
      "Epoch 27/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3850 - acc: 0.8975 - val_loss: 0.3265 - val_acc: 0.9170\n",
      "Epoch 28/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3796 - acc: 0.8987 - val_loss: 0.3216 - val_acc: 0.9179\n",
      "Epoch 29/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3746 - acc: 0.8997 - val_loss: 0.3171 - val_acc: 0.9184\n",
      "Epoch 30/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3699 - acc: 0.9006 - val_loss: 0.3128 - val_acc: 0.9194\n",
      "Epoch 31/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3654 - acc: 0.9013 - val_loss: 0.3088 - val_acc: 0.9199\n",
      "Epoch 32/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3613 - acc: 0.9020 - val_loss: 0.3050 - val_acc: 0.9206\n",
      "Epoch 33/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3573 - acc: 0.9029 - val_loss: 0.3015 - val_acc: 0.9207\n",
      "Epoch 34/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3535 - acc: 0.9039 - val_loss: 0.2980 - val_acc: 0.9217\n",
      "Epoch 35/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3499 - acc: 0.9047 - val_loss: 0.2948 - val_acc: 0.9221\n",
      "Epoch 36/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3465 - acc: 0.9056 - val_loss: 0.2917 - val_acc: 0.9233\n",
      "Epoch 37/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3432 - acc: 0.9063 - val_loss: 0.2888 - val_acc: 0.9239\n",
      "Epoch 38/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3401 - acc: 0.9070 - val_loss: 0.2860 - val_acc: 0.9233\n",
      "Epoch 39/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3371 - acc: 0.9079 - val_loss: 0.2833 - val_acc: 0.9239\n",
      "Epoch 40/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3342 - acc: 0.9085 - val_loss: 0.2807 - val_acc: 0.9256\n",
      "Epoch 41/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3314 - acc: 0.9094 - val_loss: 0.2782 - val_acc: 0.9257\n",
      "Epoch 42/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3287 - acc: 0.9098 - val_loss: 0.2758 - val_acc: 0.9269\n",
      "Epoch 43/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3262 - acc: 0.9103 - val_loss: 0.2736 - val_acc: 0.9271\n",
      "Epoch 44/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3237 - acc: 0.9110 - val_loss: 0.2714 - val_acc: 0.9279\n",
      "Epoch 45/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3213 - acc: 0.9114 - val_loss: 0.2692 - val_acc: 0.9284\n",
      "Epoch 46/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.3190 - acc: 0.9119 - val_loss: 0.2672 - val_acc: 0.9290\n",
      "Epoch 47/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.3167 - acc: 0.9126 - val_loss: 0.2652 - val_acc: 0.9296\n",
      "Epoch 48/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3145 - acc: 0.9131 - val_loss: 0.2632 - val_acc: 0.9296\n",
      "Epoch 49/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3124 - acc: 0.9136 - val_loss: 0.2614 - val_acc: 0.9304\n",
      "Epoch 50/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.3103 - acc: 0.9141 - val_loss: 0.2595 - val_acc: 0.9306\n",
      "Epoch 51/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3083 - acc: 0.9145 - val_loss: 0.2578 - val_acc: 0.9320\n",
      "Epoch 52/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.3063 - acc: 0.9151 - val_loss: 0.2561 - val_acc: 0.9309\n",
      "Epoch 53/100\n",
      "63000/63000 [==============================] - 1s 14us/step - loss: 0.3044 - acc: 0.9152 - val_loss: 0.2544 - val_acc: 0.9327\n",
      "Epoch 54/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.3026 - acc: 0.9157 - val_loss: 0.2528 - val_acc: 0.9317\n",
      "Epoch 55/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.3008 - acc: 0.9162 - val_loss: 0.2512 - val_acc: 0.9330\n",
      "Epoch 56/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.2990 - acc: 0.9168 - val_loss: 0.2496 - val_acc: 0.9327\n",
      "Epoch 57/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.2973 - acc: 0.9172 - val_loss: 0.2481 - val_acc: 0.9330\n",
      "Epoch 58/100\n",
      "63000/63000 [==============================] - 1s 15us/step - loss: 0.2956 - acc: 0.9175 - val_loss: 0.2467 - val_acc: 0.9340\n",
      "Epoch 59/100\n",
      "63000/63000 [==============================] - 1s 15us/step - loss: 0.2939 - acc: 0.9180 - val_loss: 0.2452 - val_acc: 0.9340\n",
      "Epoch 60/100\n",
      "63000/63000 [==============================] - 1s 16us/step - loss: 0.2923 - acc: 0.9184 - val_loss: 0.2439 - val_acc: 0.9337\n",
      "Epoch 61/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.2907 - acc: 0.9188 - val_loss: 0.2425 - val_acc: 0.9349\n",
      "Epoch 62/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.2891 - acc: 0.9190 - val_loss: 0.2412 - val_acc: 0.9354\n",
      "Epoch 63/100\n",
      "63000/63000 [==============================] - 1s 14us/step - loss: 0.2876 - acc: 0.9193 - val_loss: 0.2399 - val_acc: 0.9357\n",
      "Epoch 64/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.2862 - acc: 0.9199 - val_loss: 0.2385 - val_acc: 0.9361\n",
      "Epoch 65/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2847 - acc: 0.9203 - val_loss: 0.2373 - val_acc: 0.9360\n",
      "Epoch 66/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2832 - acc: 0.9206 - val_loss: 0.2360 - val_acc: 0.9363\n",
      "Epoch 67/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2818 - acc: 0.9210 - val_loss: 0.2349 - val_acc: 0.9370\n",
      "Epoch 68/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2804 - acc: 0.9215 - val_loss: 0.2336 - val_acc: 0.9369\n",
      "Epoch 69/100\n",
      "63000/63000 [==============================] - 1s 12us/step - loss: 0.2790 - acc: 0.9217 - val_loss: 0.2324 - val_acc: 0.9374\n",
      "Epoch 70/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.2777 - acc: 0.9222 - val_loss: 0.2313 - val_acc: 0.9386\n",
      "Epoch 71/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.2764 - acc: 0.9226 - val_loss: 0.2301 - val_acc: 0.9383\n",
      "Epoch 72/100\n",
      "63000/63000 [==============================] - 1s 14us/step - loss: 0.2751 - acc: 0.9231 - val_loss: 0.2290 - val_acc: 0.9394\n",
      "Epoch 73/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.2738 - acc: 0.9235 - val_loss: 0.2281 - val_acc: 0.9390\n",
      "Epoch 74/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2725 - acc: 0.9237 - val_loss: 0.2268 - val_acc: 0.9390\n",
      "Epoch 75/100\n",
      "63000/63000 [==============================] - 1s 13us/step - loss: 0.2712 - acc: 0.9242 - val_loss: 0.2258 - val_acc: 0.9396\n",
      "Epoch 76/100\n",
      "63000/63000 [==============================] - 1s 14us/step - loss: 0.2700 - acc: 0.9244 - val_loss: 0.2247 - val_acc: 0.9400\n",
      "Epoch 77/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2688 - acc: 0.9247 - val_loss: 0.2236 - val_acc: 0.9399\n",
      "Epoch 78/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2676 - acc: 0.9254 - val_loss: 0.2227 - val_acc: 0.9407\n",
      "Epoch 79/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2664 - acc: 0.9257 - val_loss: 0.2217 - val_acc: 0.9404\n",
      "Epoch 80/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2653 - acc: 0.9257 - val_loss: 0.2207 - val_acc: 0.9410\n",
      "Epoch 81/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2641 - acc: 0.9262 - val_loss: 0.2197 - val_acc: 0.9406\n",
      "Epoch 82/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2630 - acc: 0.9265 - val_loss: 0.2188 - val_acc: 0.9410\n",
      "Epoch 83/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2618 - acc: 0.9269 - val_loss: 0.2178 - val_acc: 0.9407\n",
      "Epoch 84/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2607 - acc: 0.9270 - val_loss: 0.2169 - val_acc: 0.9411\n",
      "Epoch 85/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2596 - acc: 0.9275 - val_loss: 0.2160 - val_acc: 0.9414\n",
      "Epoch 86/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2585 - acc: 0.9277 - val_loss: 0.2149 - val_acc: 0.9414\n",
      "Epoch 87/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2575 - acc: 0.9280 - val_loss: 0.2142 - val_acc: 0.9420\n",
      "Epoch 88/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2564 - acc: 0.9281 - val_loss: 0.2132 - val_acc: 0.9419\n",
      "Epoch 89/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2554 - acc: 0.9284 - val_loss: 0.2123 - val_acc: 0.9427\n",
      "Epoch 90/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2544 - acc: 0.9286 - val_loss: 0.2114 - val_acc: 0.9429\n",
      "Epoch 91/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2533 - acc: 0.9290 - val_loss: 0.2107 - val_acc: 0.9436\n",
      "Epoch 92/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2523 - acc: 0.9290 - val_loss: 0.2097 - val_acc: 0.9430\n",
      "Epoch 93/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2513 - acc: 0.9293 - val_loss: 0.2089 - val_acc: 0.9431\n",
      "Epoch 94/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2503 - acc: 0.9297 - val_loss: 0.2082 - val_acc: 0.9439\n",
      "Epoch 95/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2494 - acc: 0.9298 - val_loss: 0.2073 - val_acc: 0.9437\n",
      "Epoch 96/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2484 - acc: 0.9301 - val_loss: 0.2065 - val_acc: 0.9441\n",
      "Epoch 97/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2474 - acc: 0.9305 - val_loss: 0.2057 - val_acc: 0.9441\n",
      "Epoch 98/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2465 - acc: 0.9307 - val_loss: 0.2049 - val_acc: 0.9443\n",
      "Epoch 99/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2456 - acc: 0.9310 - val_loss: 0.2040 - val_acc: 0.9446\n",
      "Epoch 100/100\n",
      "63000/63000 [==============================] - 1s 11us/step - loss: 0.2446 - acc: 0.9312 - val_loss: 0.2035 - val_acc: 0.9449\n",
      "Loss and accuracy for MNIST Test data: \n",
      "10000/10000 [==============================] - 0s 11us/step\n",
      "0.24253741892278194\n",
      "0.9308\n",
      "Loss and accuracy for USPS data: \n",
      "19999/19999 [==============================] - 0s 13us/step\n",
      "2.682430596451526\n",
      "0.3801690084414814\n"
     ]
    }
   ],
   "source": [
    "#(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "num_classes=10\n",
    "image_vector_size=28*28\n",
    "#x_train = x_train.reshape(x_train.shape[0], image_vector_size)\n",
    "#x_test = x_test.reshape(x_test.shape[0], image_vector_size)\n",
    "x_train = Mtraining_data;\n",
    "u_x_test = USPSMat;\n",
    "y_train = keras.utils.to_categorical(Mtraining_target, num_classes);\n",
    "u_y_test = keras.utils.to_categorical(USPSTar, num_classes);\n",
    "x_test = Mtest_data;\n",
    "y_test = keras.utils.to_categorical(Mtest_target, num_classes);\n",
    "#print(np.shape(x_test))\n",
    "#print(np.shape(y_test))\n",
    "#print(np.shape(x_train))\n",
    "#print(np.shape(y_train))\n",
    "#print(y_train[0])\n",
    "image_size = 784\n",
    "model = Sequential()\n",
    "model.add(Dense(units=32, activation='sigmoid', input_shape=(image_size,)))\n",
    "model.add(Dense(units=num_classes,activation='softmax'))\n",
    "model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, batch_size=128, epochs=100, verbose=True,validation_split=.1)\n",
    "print(\"Loss and accuracy for MNIST Test data: \");\n",
    "loss,accuracy = model.evaluate(x_test, y_test, verbose = True);\n",
    "print(loss)\n",
    "print(accuracy)\n",
    "print(\"Loss and accuracy for USPS data: \");\n",
    "loss,accuracy = model.evaluate(u_x_test, u_y_test, verbose = True);\n",
    "print(loss)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]Accuracy for SVM on MNIST Data :\n",
      "0.9836\n"
     ]
    }
   ],
   "source": [
    "svm_x_train = Mtraining_data;\n",
    "svm_y_train = Mtraining_target;\n",
    "svm_x_test = Mtest_data;\n",
    "svm_y_test = Mtest_target;\n",
    "svm_u_x_test = USPSMat;\n",
    "svm_u_y_test = USPSTar;\n",
    "#print(np.shape(x_train));\n",
    "#print(np.shape(y_train));\n",
    "#print(np.shape(x_test));\n",
    "#print(np.shape(y_test));\n",
    "classifier1 = SVC(kernel='rbf', C=2, gamma = 0.05, verbose =1);\n",
    "classifier1.fit(svm_x_train, svm_y_train)\n",
    "accuracy = classifier1.score(svm_x_test,svm_y_test);\n",
    "predict = classifier1.predict(svm_x_test);\n",
    "print(\"Accuracy for SVM on MNIST Data :\")\n",
    "print(accuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for USPS dataset\n",
      "0.27391369568478424\n"
     ]
    }
   ],
   "source": [
    "accuracy1 = classifier1.score(svm_u_x_test,svm_u_y_test);\n",
    "print(\"Accuracy for USPS dataset\")\n",
    "print(accuracy1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:   40.9s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for MNIST dataset: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9712\n",
      "Accuract for USPS dataset: \n",
      "0.39056952847642384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "rf_x_train = Mtraining_data;\n",
    "rf_y_train = Mtraining_target;\n",
    "rf_x_test = Mtest_data;\n",
    "rf_y_test = Mtest_target;\n",
    "rf_u_x_test = USPSMat;\n",
    "rf_u_y_test = USPSTar;\n",
    "classifier2 = RandomForestClassifier(n_estimators=100, verbose=1);\n",
    "classifier2.fit(rf_x_train, rf_y_train)\n",
    "print(\"Accuracy for MNIST dataset: \");\n",
    "accuracy = classifier2.score(rf_x_test,rf_y_test);\n",
    "print(accuracy);\n",
    "print(\"Accuract for USPS dataset: \");\n",
    "accuracy = classifier2.score(rf_u_x_test,rf_u_y_test);\n",
    "print(accuracy);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrices and Majority Voting Implementation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1 = open('softmax_predictions','rb');\n",
    "smp = pickle.load(f1,encoding='latin1');\n",
    "f1.close();\n",
    "#print(smp);\n",
    "#print(smp);\n",
    "smp = np.array(smp);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f2 = open('dnn_predictions','rb');\n",
    "dnnp = pickle.load(f2,encoding = 'latin1');\n",
    "f2.close();\n",
    "dnnp = np.array(dnnp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f3 = open('SVM_predictions','rb');\n",
    "svmp = pickle.load(f3, encoding='latin1');\n",
    "f3.close();\n",
    "#print(svmp);\n",
    "svmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 5, 6])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f4 = open('random_forest_predictions','rb');\n",
    "rfp = pickle.load(f4,encoding=\"latin1\");\n",
    "f4.close();\n",
    "rfp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Softmax regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 935    0    5    5    0   15   11    1    7    1]\n",
      " [   0 1095    4    6    0    5    3    0   22    0]\n",
      " [  18    8  865   20   19    0   17   23   51   11]\n",
      " [   4    4   21  878    0   47    5   16   21   14]\n",
      " [   1    2    5    2  864    5   26    1    9   67]\n",
      " [  18    6    8   62   13  682   19    7   61   16]\n",
      " [  15    6   13    2   16   16  879    1   10    0]\n",
      " [   2   18   38    4    9    1    0  910    3   43]\n",
      " [   5   16   10   46   19   42   13   15  788   20]\n",
      " [   9    7    5    8   63   20    1   28   14  854]]\n"
     ]
    }
   ],
   "source": [
    "cf1 = confusion_matrix(Mtest_target, smp)\n",
    "print(cf1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for DNN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 959    0    2    2    0    5    8    1    3    0]\n",
      " [   0 1112    3    2    0    1    4    2   11    0]\n",
      " [   9    4  937   14   17    0   13   11   25    2]\n",
      " [   3    1   16  926    0   24    2   17   15    6]\n",
      " [   1    1    5    0  921    2    9    2    6   35]\n",
      " [   9    3    2   41    5  784   14    4   21    9]\n",
      " [  13    3    5    0   10   11  913    1    2    0]\n",
      " [   3   10   25    4    6    0    0  950    2   28]\n",
      " [   8    9    5   16    9   22   14    9  877    5]\n",
      " [  10    7    2   11   38   10    0   12    6  913]]\n"
     ]
    }
   ],
   "source": [
    "cf2 = confusion_matrix(Mtest_target, dnnp)\n",
    "print(cf2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for SVM :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 973    0    1    0    0    2    1    1    2    0]\n",
      " [   0 1127    3    1    0    1    1    1    1    0]\n",
      " [   4    0 1015    0    1    0    0    6    6    0]\n",
      " [   0    0    2  995    0    3    0    6    4    0]\n",
      " [   0    0    3    0  966    0    4    0    2    7]\n",
      " [   2    0    0    5    1  878    2    1    2    1]\n",
      " [   4    2    0    0    2    3  946    0    1    0]\n",
      " [   0    3   10    1    1    0    0 1004    2    7]\n",
      " [   1    0    1    3    1    2    0    2  961    3]\n",
      " [   3    3    2    6   10    2    0    6    6  971]]\n"
     ]
    }
   ],
   "source": [
    "cf3 = confusion_matrix(Mtest_target, svmp)\n",
    "print(cf3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix for Random Forest :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 969    0    0    0    0    4    3    1    2    1]\n",
      " [   0 1122    3    3    0    1    3    1    2    0]\n",
      " [   6    0  999    4    2    0    4    9    8    0]\n",
      " [   1    0   10  972    0    9    0    9    7    2]\n",
      " [   1    0    1    0  955    0    5    0    3   17]\n",
      " [   2    1    0   12    2  860    5    2    5    3]\n",
      " [   5    3    0    0    2    3  942    0    3    0]\n",
      " [   2    2   17    1    0    1    0  993    3    9]\n",
      " [   5    0    3   11    5    3    2    4  933    8]\n",
      " [   5    4    1   11    9    3    1    4   10  961]]\n"
     ]
    }
   ],
   "source": [
    "cf4 = confusion_matrix(Mtest_target, rfp)\n",
    "print(cf4);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Majority Voting :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "L = [];\n",
    "E = [];\n",
    "for i in range(0,10000):\n",
    "    L.append(smp[i]);\n",
    "    L.append(dnnp[i]);\n",
    "    L.append(svmp[i]);\n",
    "    L.append(rfp[i]);\n",
    "    #print(mode(L));\n",
    "    #E.append(max(L, key = L.count))\n",
    "    try:\n",
    "        E.append(mode(L));\n",
    "    except:\n",
    "        #print(\"error in  index: \"+str(i));\n",
    "        E.append(svmp[i]);\n",
    "        continue\n",
    "    L=[];\n",
    "print(np.shape(E));\n",
    "#print(E[9698]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9738\n",
      "97.38\n"
     ]
    }
   ],
   "source": [
    "Eaccuracy = 0;\n",
    "count = 0\n",
    "k=0;\n",
    "for i in range(0,10000):\n",
    "    if(Mtest_target[i] == E[i]):\n",
    "        count = count+1;\n",
    "print(count);\n",
    "Eaccuracy = float((count*100)/len(E));\n",
    "print(\"Accuracy for Majority voting\");\n",
    "print(Eaccuracy);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
